{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Frequency Distributions with NLTK (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab instructor: Zeyang(Jenny) Gong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel: Python 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under blackboard, you can find the following documents: \n",
    "\n",
    "•\tInstallation instruction \n",
    "\n",
    "•\tLab1.doc (tutorial that you can go over by yourself after the lab) \n",
    "\n",
    "•\tLab1.ipynb (material that will be present during the lab)\n",
    "\n",
    "•\tFrequency.py (code to create frequency distribution) \n",
    "\n",
    "•\tFrequency without stopwords.py (code to create frequency distribution without stop words) \n",
    "\n",
    "•\tInput.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to follow the instructor during the lab, please get your jupyter notebook ready before the lab. The installation instruction has been posted on the Blackboard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be learning how to use the Natural Language Toolkit (NLTK) module to perform basic text analysis. \n",
    "\n",
    "NLTK is a Python add-on package which has multiple useful functions for analyzing text files. One key component of text analysis is finding keyword frequency distributions.\n",
    "\n",
    "A frequency distribution can be shown as a table containing how many times within a document different words appear. To directly quote the NLTK book, “it’s called a \"distribution\" because it tells us how the total number of word tokens in the text are distributed across the vocabulary items. Since we often need frequency distributions in language processing, NLTK provides built-in support for them” (http://www.nltk.org/book/ch01.html). \n",
    "\n",
    "Let’s get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab1:\n",
    "1. String Manipulation \n",
    "2. tokenization \n",
    "3. stop words removal\n",
    "\n",
    "### Lab2:\n",
    "3. stemming \n",
    "4. POS\n",
    "5. N-grams\n",
    "6. Tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = '  Hello '\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = s.strip()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#string indexing \n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 'http:\\\\www.abc.com'\n",
    "s.startswith('http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 'I love coding'\n",
    "s_list = s.split() #default deliminator is ' '(space) \n",
    "s_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_string = '$$'.join(s_list)\n",
    "new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s.replace('$$',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "#read file from local \n",
    "f = open('input.txt','rU')\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = raw.replace('\\n',' ') \n",
    "raw = raw.decode('utf8') #decode raw text by utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change all tokens into lower case \n",
    "words1 = [w.lower() for w in tokens]   #list comprehension \n",
    "\n",
    "#only keep text words, no numbers \n",
    "words2 = [w for w in words1 if w.isalpha()]\n",
    "\n",
    "#encode the raw token list by utf-8\n",
    "words3 = [w.encode('utf8') for w in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#another way to create a new list: \n",
    "words1 = []\n",
    "for w in tokens: \n",
    "    words1.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate a frequency dictionary for all tokens \n",
    "freq = FreqDist(words3)\n",
    "\n",
    "#sort the frequency list in descending order\n",
    "sorted_freq = sorted(freq.items(),key = lambda k:k[1], reverse = True)\n",
    "sorted_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english') #use the NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#only keep the words that not in nltk stopwords word list\n",
    "words_nostopwords = [w.encode('utf8') for w in words2 if w not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate a frequency dictionary for all tokens \n",
    "freq_nostw = FreqDist(words_nostopwords)\n",
    "\n",
    "#sort the frequency list in decending order\n",
    "sorted_freq_nostw = sorted(freq_nostw.items(),key = lambda k:k[1], reverse = True)\n",
    "sorted_freq_nostw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nostw.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('output.txt','a') as outfile:\n",
    "    for line in sorted_freq_nostw:\n",
    "        outfile.write(str(line[0])+'\\t'+str(line[1])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All done!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}